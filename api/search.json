[{"id":"c3bb980dd99e420748682f824a1ae74e","title":"手写数字识别","content":"餐前祷告怎么才能让你的电脑有那么一丢丢的小智能呢，现在好像有好多方法，但是那些高大上的方法，好像我们现在难以自己去实现，不过嘛万丈高楼平地起，我们可以用我们的聪明才智来实现一些小小的智能。\n基础理论  那么我们就从手写数字识别开始吧！\n  我们都知道人工智能三要素是什么呢？数据、算法、计算资源。现在呢我们需要第一个东西“数据”。那数据在哪呢，原遇到，等等，楼门口来前辈们为我们贴心的准备好了，他就藏在一个叫keras的库里面。那么数据解决了算法呢？算法自在心中，下面就让我来细细说说吧。\n  这里呢我们就用一个结构简单，且易于理解的CNN卷积神经网络。什么！CNN！不就是那个图里面一层一层好复杂的东西吗？比如下面这个东西。莫怕莫怕，且听我细细道来\n\n   其实呢，抛去那些晦涩难懂的概念，细说CNN的构成就那么几层罢了，卷积层、激活函数、池化层、全连接层。我们这里打算仅搭建四层，进行这个让我们的电脑成为一个“小超人”这个神圣的任务。第一层：卷积层 、第二层：卷积层、 第三层：全连接层、 第四层：输出层。\n\n  我们的数据集呢「1」，文末介绍。我们建立这些层的原因是什么呢？这要从卷积神经网络的原理说起，即对图片的特征进行提取，我们现在第二个卷积层窗口大小为5×5，对32张图像求卷积产生64个特征图，参数个数是 5×5×32×64=51200个权值加上64个偏置。\n\n   池化计算是在卷积层中进行的，使用2×2，步长为2的池化窗口做池化计算，池化后得到64张7×7的特征图。特征图长宽都变成了之前的1/2。\n\n   第三层是全连接层，为池化层的结果做池化计算，池化后得到特征图。\n\n    第四层是输出层，输出预测值。\n\n   **特征图数量越多**说明卷积网络提取的**特征数量越多**，如果特征图数量设置得**太少**容易出现**欠拟合**，如果特征图数量设置得**太多**容易出现**过拟合**，所以需要设置为合适的数值。 \n\n开始搭建神经网络引入数据集这里呢我们引入数据集是一个非常简单的事情，只需要导入keras.datasets下的mnist即可。\nmnist &#x3D; tf.keras.datasets.mnist\n(train_data,traini_target), (test_data,test_target) &#x3D; mnist.load_data() \n\n\n\n数据预处理这里我们要使用TensorFlow来进行后续操作，所以需要先对数据进行处理。\ntrain_data &#x3D; train_data.reshape(-1, 28, 28, 1)\ntest_data &#x3D; test_data.reshape(-1, 28, 28, 1)\n\ntrain_data &#x3D; train_data&#x2F;255.0\ntest_data &#x3D; test_data&#x2F;255.0\n\ntrain_target &#x3D; tf.keras.utils.to_categorical(train_target, num_classes&#x3D;10)\ntest_target &#x3D; tf.keras.utils.to_categorical(test_target, num_classes&#x3D;10)    #10种结果\n\n搭建网络卷积层的搭建这一层主要是由卷积层+池化层组成，在tensorflow中为我们直接提供了函数。\nmodel.add(Convolution2D(input_shape &#x3D; (28,28,1), filters &#x3D; 32, kernel_size &#x3D; 5, strides &#x3D; 1, padding &#x3D; &#39;same&#39;, activation &#x3D; &#39;relu&#39;))\n\nmodel.add(MaxPooling2D(pool_size &#x3D; 2, strides &#x3D; 2, padding &#x3D; &#39;same&#39;,))\n\n第二个卷积层model.add(Convolution2D(64, 5, strides&#x3D;1, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;))\n\nmodel.add(MaxPooling2D(2, 2, &#39;same&#39;))\n\nmodel.add(Flatten())\n\n第一个全连接层model.add(Dense(1024,activation &#x3D; &#39;relu&#39;))\nmodel.add(Dropout(0.5))\n\n第二个全连接层（输出层）model.add(Dense(10, activation&#x3D;&#39;softmax&#39;))\n\n编译model.compile(optimizer&#x3D;Adam(lr&#x3D;1e-4), loss&#x3D;&#39;categorical_crossentropy&#39;, metrics&#x3D;[&#39;accuracy&#39;])\n\n训练与保存model.fit(train_data, train_target, batch_size&#x3D;64, epochs&#x3D;10, validation_data&#x3D;(test_data, test_target))\nmodel.save(&quot;mnist.h5&quot;)\n\n# 手写数字识别 -- CNN神经网络训练\nimport os\nos.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;]&#x3D;&#39;2&#39;\n \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\nfrom tensorflow.keras.optimizers import Adam\n \n# 1、载入数据\nmnist &#x3D; tf.keras.datasets.mnist\n(train_data, train_target), (test_data, test_target) &#x3D; mnist.load_data()\n \n# 2、改变数据维度\ntrain_data &#x3D; train_data.reshape(-1, 28, 28, 1)\ntest_data &#x3D; test_data.reshape(-1, 28, 28, 1)\n# 注：在TensorFlow中，在做卷积的时候需要把数据变成4维的格式\n# 这4个维度分别是：数据数量，图片高度，图片宽度，图片通道数\n \n# 3、归一化（有助于提升训练速度）\ntrain_data &#x3D; train_data&#x2F;255.0\ntest_data &#x3D; test_data&#x2F;255.0\n \n# 4、独热编码\ntrain_target &#x3D; tf.keras.utils.to_categorical(train_target, num_classes&#x3D;10)\ntest_target &#x3D; tf.keras.utils.to_categorical(test_target, num_classes&#x3D;10)    #10种结果\n \n# 5、搭建CNN卷积神经网络\nmodel &#x3D; Sequential()\n# 5-1、第一层：卷积层+池化层\n# 第一个卷积层\nmodel.add(Convolution2D(input_shape &#x3D; (28,28,1), filters &#x3D; 32, kernel_size &#x3D; 5, strides &#x3D; 1, padding &#x3D; &#39;same&#39;, activation &#x3D; &#39;relu&#39;))\n#         卷积层         输入数据                  滤波器数量      卷积核大小        步长          填充数据(same padding)  激活函数\n# 第一个池化层 # pool_size\nmodel.add(MaxPooling2D(pool_size &#x3D; 2, strides &#x3D; 2, padding &#x3D; &#39;same&#39;,))\n#         池化层(最大池化) 池化窗口大小   步长          填充方式\n \n# 5-2、第二层：卷积层+池化层\n# 第二个卷积层\nmodel.add(Convolution2D(64, 5, strides&#x3D;1, padding&#x3D;&#39;same&#39;, activation&#x3D;&#39;relu&#39;))\n# 64:滤波器个数      5:卷积窗口大小\n# 第二个池化层\nmodel.add(MaxPooling2D(2, 2, &#39;same&#39;))\n \n# 5-3、扁平化 （相当于把(64,7,7,64)数据-&gt;(64,7*7*64)）\nmodel.add(Flatten())\n \n# 5-4、第三层：第一个全连接层\nmodel.add(Dense(1024, activation &#x3D; &#39;relu&#39;))\nmodel.add(Dropout(0.5))\n \n# 5-5、第四层：第二个全连接层（输出层）\nmodel.add(Dense(10, activation&#x3D;&#39;softmax&#39;))\n# 10：输出神经元个数\n \n# 6、编译\nmodel.compile(optimizer&#x3D;Adam(lr&#x3D;1e-4), loss&#x3D;&#39;categorical_crossentropy&#39;, metrics&#x3D;[&#39;accuracy&#39;])\n#            优化器(adam)               损失函数(交叉熵损失函数)            标签\n \n# 7、训练\nmodel.fit(train_data, train_target, batch_size&#x3D;64, epochs&#x3D;10, validation_data&#x3D;(test_data, test_target))\n \n# 8、保存模型\nmodel.save(&#39;mnist.h5&#39;)\n\n","slug":"手写数字识别","date":"2022-10-26T07:25:25.481Z","categories_index":"算法","tags_index":"人工智能","author_index":"Gaoth"},{"id":"33acdddb35650af797ecddd2d38891a7","title":"要好好学习天天向上哟","content":"鸽巢原理进行排序https://leetcode.cn/problems/missing-two-lcci/\n123for (int i=0 ; i&lt;nums.size() ; i++)&#123;\twhile(nums[i] != -1 &amp;&amp; nums[i] != i+1) swap(nums[i] , nums[nums[i]-1]);&#125;\n\n说明：此模板用于对有规律的数进行排序，核心思想是一个萝卜一个坑。\n桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面放不少于两个苹果。这一现象就是我们所说的“抽屉原理”。 抽屉原理的一般含义为：“如果每个抽屉代表一个集合，每一个苹果就可以代表一个元素，假如有n+1个元素放到n个集合中去，其中必定有一个集合里至少有两个元素。” 抽屉原理有时也被称为鸽巢原理。本质上是对哈希定址法的改变。\n排序算法（归并排序）这里放题目\n对于归并排序我是这样理解的，对于一个未排序的数组，我们可以将其对半分开，分开后再将其按顺序合并，那么具体的合并流程呢？首先需要先创建一个新的数组（这个数组容量为两个子数组容量的和），然后将两个子数组按大小排入这个新数组中，因为我们是将小块合并所以保证在前面的数组是按照规则排序的，那么后面的只需要比较两个小数组的第一个值即可保证不错位。\n\n\nPython排序进阶（工具）https://leetcode.cn/problems/largest-number/\n我们知道Python中排序有sorted(_iterable, key, reverse),而这里我们着重来说这个“key”，这个“key”是让我们sorted()函数真正好用的关键。\n针对key我们可以用lambda:或写函数来改变，也可以通过functools模块中的cmp_to_key来对自定义的cmp函数进行包装，然后就能赋值给sorted函数的关键字参数key，来间接实现Python2中cmp函数用于排序的效果。\nPython排序进阶(2)（工具）https://leetcode.cn/problems/advantage-shuffle/\nPython中还提供了一个排序的库（sortedcontainers）。\n这个库提供了三个类：SortedList 、SortedDict、SortedSet。我们可以直接声明一个SortedList对象，这个对象会直接对列表中的值进行排序，\n123456import sortedcontainerst = sortedcontainers.SortedList([4, 2, 5, 3, 1])print(t)&gt;&gt;SortedList([1, 2, 3, 4, 5])\n\n常见的方法：add()、remove()、discard()、pop()、bisect_left()、count()。\n\n\n\n方法\n时间复杂度\n说明\n\n\n\nadd()\n近似O(logn)\n向SortedList()中添加元素\n\n\nupdate(iterable)\n近似O(k*logn)\n向SortedList()中添加列表\n\n\nclear()\nO(n)\n删除所以元素\n\n\ndiscard(value)\n近似O(logn)\n删除单个元素（元素可不存在）\n\n\nremove()\n近似O(logn)\n删除单个元素（元素不存在报错）\n\n\npop(index&#x3D;-1)\n近似O(logn)\n同列表pop\n\n\nbisect_left(value)\n近似O(logn)\n找出元素位置\n\n\ncount()\n近似O(logn)\n查找个数\n\n\nindex()\n近似O(logn)\n同列表index\n\n\nSortedList的排序方法是系统默认的，我们也可以设置排序方法，比如数值从大到小排序\n123456789from sortedcontainers import SortedListfrom operator import neg test_sl = SortedList([3,5,1,2,7,6,4], key=neg) print(test_sl) output:SortedKeyList([7, 6, 5, 4, 3, 2, 1], key=&lt;built-in function neg&gt;)\n\n1234567from sortedcontainers import SortedList test_str = SortedList([&quot;1&quot;, &quot;431&quot;, &quot;34&quot;], key=lambda item:len(item))print(test_str) output:SortedKeyList([&#x27;1&#x27;, &#x27;34&#x27;, &#x27;431&#x27;], key=&lt;function &lt;lambda&gt; at 0x7fb883b36820&gt;)\n\n\n\n回溯（全排列）模板https://leetcode.cn/problems/permutations/\n题目分析：就这道题目而言，因其不包含重复数字，故不需考虑重复问题，那么怎么样实现全排列呢？我们从小都知道，全排列要有条理的从原列表中一个个拿出，而拿出后我们应该将这个元素去掉！所以我们就可以有思路了。本质上我们应将数字从列表中一个个拿出这样我们可以用一层循环遍历。用“我”的浅薄装逼的表示即为：\n$f(x+1)&#x3D;f(x)+d[i]$\n解释：$f(x)$为我们要组成的结果，$d[i]$为列表中的元素。列表元素是一遍遍减少的！这个是重要的\n代码：1234567891011class Solution:    def permute(self, nums: List[int]) -&gt; List[List[int]]:        res = []        def backtracking(nums, t):            if not nums:                res.append(t)                return            for i in range(len(nums)):                backtracking(nums[:i]+nums[i+1:],t+[nums[i]])        backtracking(nums,[])        return res\n\n再说回来回溯法：采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案。回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况：\n找到一个可能存在的正确的答案；在尝试了所有可能的分步方法后宣告该问题没有答案。\n回溯算法关键在于：不合适就退回上一步，然后通过约束条件, 减少时间复杂度。其和深度搜索算法是有些相似之处的。\n","slug":"刷题不","date":"2022-10-25T12:30:21.151Z","categories_index":"算法","tags_index":"算法","author_index":"Gaoth"}]